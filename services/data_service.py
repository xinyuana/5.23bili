import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import uuid
from services.elasticsearch_service import ElasticsearchService

class DataService:
    def __init__(self):
        self.es_service = ElasticsearchService()
        self.user_project_mapping = None
        
    def load_user_project_mapping(self):
        """åŠ è½½ç”¨æˆ·IDåˆ°é¡¹ç›®çš„æ˜ å°„å…³ç³»"""
        if self.user_project_mapping is not None:
            return self.user_project_mapping
            
        account_file = 'è´¦å·å¤§æ•´åˆ3.2xlsx_å·²æå–UID.csv'
        self.user_project_mapping = {}
        
        if os.path.exists(account_file):
            try:
                df = pd.read_csv(account_file, low_memory=False)
                
                # å»ºç«‹ç”¨æˆ·IDåˆ°é¡¹ç›®çš„æ˜ å°„
                for _, row in df.iterrows():
                    user_id = str(row.get('ç”¨æˆ·ID', '')).strip()
                    project = str(row.get('å·¥ä½œè¡¨', '')).strip()
                    
                    if user_id and project and user_id != 'nan' and project != 'nan':
                        self.user_project_mapping[user_id] = project
                
                print(f"âœ… æˆåŠŸåŠ è½½ {len(self.user_project_mapping)} ä¸ªç”¨æˆ·çš„é¡¹ç›®æ˜ å°„")
                
                # æ‰“å°é¡¹ç›®ç»Ÿè®¡
                project_counts = {}
                for project in self.user_project_mapping.values():
                    project_counts[project] = project_counts.get(project, 0) + 1
                
                print("ğŸ“Š é¡¹ç›®åˆ†å¸ƒ:")
                for project, count in sorted(project_counts.items()):
                    print(f"   {project}: {count} ä¸ªç”¨æˆ·")
                    
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç”¨æˆ·é¡¹ç›®æ˜ å°„å¤±è´¥: {str(e)}")
                
        return self.user_project_mapping
    
    def get_user_project(self, user_id):
        """æ ¹æ®ç”¨æˆ·IDè·å–é¡¹ç›®åç§°"""
        if self.user_project_mapping is None:
            self.load_user_project_mapping()
            
        return self.user_project_mapping.get(str(user_id), 'æœªåˆ†ç±»é¡¹ç›®')
        
    def import_data_async(self, import_params):
        """å¼‚æ­¥å¯¼å…¥æ•°æ®ï¼ˆè¿”å›ä»»åŠ¡IDï¼‰"""
        task_id = str(uuid.uuid4())
        
        # åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥ä½¿ç”¨Celeryç­‰ä»»åŠ¡é˜Ÿåˆ—
        # ç°åœ¨æˆ‘ä»¬ç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•è¿›è¡Œæ¼”ç¤º
        try:
            self.import_data_sync(import_params)
            return task_id
        except Exception as e:
            print(f"æ•°æ®å¯¼å…¥å¤±è´¥: {str(e)}")
            raise
    
    def import_data_sync(self, import_params):
        """åŒæ­¥å¯¼å…¥æ•°æ®"""
        data_type = import_params.get('data_type', 'all')  # 'videos', 'comments', 'all'
        
        # åŠ è½½ç”¨æˆ·é¡¹ç›®æ˜ å°„
        self.load_user_project_mapping()
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        self.es_service.create_indices()
        
        if data_type in ['videos', 'all']:
            self.import_videos()
        
        if data_type in ['comments', 'all']:
            self.import_comments()
    
    def import_videos(self):
        """å¯¼å…¥è§†é¢‘æ•°æ®"""
        video_files = [
            '1747748467790_dbexport_209215447/2025-05-20-21-41-08_EXPORT_CSV_19274722_345_bilibili_video_0.csv'
        ]
        
        for video_file in video_files:
            if os.path.exists(video_file):
                print(f"æ­£åœ¨å¯¼å…¥è§†é¢‘æ–‡ä»¶: {video_file}")
                
                # åˆ†å—è¯»å–å¤§æ–‡ä»¶
                chunk_size = 1000
                for chunk in pd.read_csv(video_file, chunksize=chunk_size, low_memory=False):
                    self.process_video_chunk(chunk)
    
    def process_video_chunk(self, df):
        """å¤„ç†è§†é¢‘æ•°æ®å—"""
        bulk_data = []
        
        for _, row in df.iterrows():
            # æ ¹æ®ç”¨æˆ·IDè·å–é¡¹ç›®
            user_id = str(row.get('user_id', ''))
            project_id = self.get_user_project(user_id)
            
            # æ¸…ç†å’Œè½¬æ¢æ•°æ®
            doc = {
                'id': str(row.get('id', '')),
                'user_id': user_id,
                'nickname': str(row.get('nickname', '')),
                'avatar': str(row.get('avatar', '')),
                'add_ts': self.convert_timestamp(row.get('add_ts')),
                'last_modify_ts': self.convert_timestamp(row.get('last_modify_ts')),
                'video_id': str(row.get('video_id', '')),
                'video_type': str(row.get('video_type', 'video')),
                'title': str(row.get('title', '')),
                'desc': str(row.get('desc', '')),
                'create_time': self.convert_timestamp(row.get('create_time')),
                'liked_count': self.safe_int(row.get('liked_count', 0)),
                'video_play_count': self.safe_int(row.get('video_play_count', 0)),
                'video_danmaku': self.safe_int(row.get('video_danmaku', 0)),
                'video_comment': self.safe_int(row.get('video_comment', 0)),
                'video_url': str(row.get('video_url', '')),
                'video_cover_url': str(row.get('video_cover_url', '')),
                'source_keyword': str(row.get('source_keyword', '')),
                'project_id': project_id
            }
            
            # æ„å»ºæ‰¹é‡æ’å…¥æ•°æ®
            bulk_data.append({
                '_index': self.es_service.video_index,
                '_id': doc['video_id'],  # ä½¿ç”¨video_idä½œä¸ºæ–‡æ¡£IDå®ç°å»é‡
                '_source': doc
            })
        
        # æ‰¹é‡æ’å…¥Elasticsearch
        if bulk_data:
            self.bulk_insert(bulk_data)
    
    def import_comments(self):
        """å¯¼å…¥è¯„è®ºæ•°æ®"""
        comment_files = [
            '1747748467790_dbexport_209215447/2025-05-20-21-41-11_EXPORT_CSV_19274722_637_bilibili_video_comment_1.csv'
        ]
        
        # é¦–å…ˆæ„å»ºè§†é¢‘IDåˆ°é¡¹ç›®çš„æ˜ å°„
        self.build_video_project_mapping()
        
        for comment_file in comment_files:
            if os.path.exists(comment_file):
                print(f"æ­£åœ¨å¯¼å…¥è¯„è®ºæ–‡ä»¶: {comment_file}")
                
                # åˆ†å—è¯»å–å¤§æ–‡ä»¶
                chunk_size = 1000
                for chunk in pd.read_csv(comment_file, chunksize=chunk_size, low_memory=False):
                    self.process_comment_chunk(chunk)
    
    def build_video_project_mapping(self):
        """æ„å»ºè§†é¢‘IDåˆ°é¡¹ç›®çš„æ˜ å°„å…³ç³»ï¼ŒåŒæ—¶æ„å»ºè§†é¢‘è¯¦ç»†ä¿¡æ¯æ˜ å°„"""
        if hasattr(self, 'video_project_mapping'):
            return
            
        self.video_project_mapping = {}
        self.video_info_mapping = {}  # æ·»åŠ è§†é¢‘è¯¦ç»†ä¿¡æ¯æ˜ å°„
        video_files = [
            '1747748467790_dbexport_209215447/2025-05-20-21-41-08_EXPORT_CSV_19274722_345_bilibili_video_0.csv'
        ]
        
        print("ğŸ”— æ„å»ºè§†é¢‘-é¡¹ç›®æ˜ å°„å…³ç³»...")
        
        for video_file in video_files:
            if os.path.exists(video_file):
                # åˆ†å—è¯»å–è§†é¢‘æ–‡ä»¶
                chunk_size = 5000
                for chunk in pd.read_csv(video_file, chunksize=chunk_size, low_memory=False):
                    for _, row in chunk.iterrows():
                        video_id = str(row.get('video_id', ''))
                        user_id = str(row.get('user_id', ''))
                        project_id = self.get_user_project(user_id)
                        
                        if video_id and video_id != 'nan':
                            self.video_project_mapping[video_id] = project_id
                            # åŒæ—¶ä¿å­˜è§†é¢‘è¯¦ç»†ä¿¡æ¯
                            self.video_info_mapping[video_id] = {
                                'video_title': str(row.get('title', '')),
                                'video_url': str(row.get('video_url', '')),
                                'video_uploader_nickname': str(row.get('nickname', '')),
                                'video_uploader_uid': str(row.get('user_id', ''))
                            }
        
        print(f"âœ… æ„å»ºäº† {len(self.video_project_mapping)} ä¸ªè§†é¢‘çš„é¡¹ç›®æ˜ å°„")
        print(f"âœ… æ„å»ºäº† {len(self.video_info_mapping)} ä¸ªè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯æ˜ å°„")
    
    def get_video_project(self, video_id):
        """æ ¹æ®è§†é¢‘IDè·å–é¡¹ç›®åç§°"""
        if hasattr(self, 'video_project_mapping'):
            return self.video_project_mapping.get(str(video_id), 'æœªåˆ†ç±»é¡¹ç›®')
        else:
            return 'æœªåˆ†ç±»é¡¹ç›®'

    def process_comment_chunk(self, df):
        """å¤„ç†è¯„è®ºæ•°æ®å—"""
        bulk_data = []
        
        for _, row in df.iterrows():
            # æ ¹æ®è§†é¢‘IDè·å–é¡¹ç›®ï¼ˆè€Œä¸æ˜¯è¯„è®ºè€…çš„user_idï¼‰
            video_id = str(row.get('video_id', ''))
            project_id = self.get_video_project(video_id)
            
            # è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
            video_info = self.get_video_info(video_id)
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¯„è®º
            parent_comment_id = str(row.get('parent_comment_id', ''))
            is_main_comment = (parent_comment_id == '' or parent_comment_id == '0')
            
            # æ¸…ç†å’Œè½¬æ¢æ•°æ®
            doc = {
                'id': str(row.get('id', '')),
                'user_id': str(row.get('user_id', '')),  # è¿™æ˜¯è¯„è®ºè€…çš„ID
                'nickname': str(row.get('nickname', '')),
                'avatar': str(row.get('avatar', '')),
                'add_ts': self.convert_timestamp(row.get('add_ts')),
                'last_modify_ts': self.convert_timestamp(row.get('last_modify_ts')),
                'comment_id': str(row.get('comment_id', '')),
                'video_id': video_id,  # è¿™æ˜¯è§†é¢‘çš„ID
                'content': str(row.get('content', '')),
                'create_time': self.convert_timestamp(row.get('create_time')),
                'sub_comment_count': self.safe_int(row.get('sub_comment_count', 0)),
                'parent_comment_id': parent_comment_id,
                'like_count': self.safe_int(row.get('like_count', 0)),
                'project_id': project_id,  # åŸºäºè§†é¢‘çš„é¡¹ç›®ï¼Œè€Œä¸æ˜¯è¯„è®ºè€…çš„é¡¹ç›®
                
                # æ·»åŠ è§†é¢‘ç›¸å…³ä¿¡æ¯
                'video_title': video_info.get('video_title', ''),
                'video_url': video_info.get('video_url', ''),
                'video_uploader_nickname': video_info.get('video_uploader_nickname', ''),
                'video_uploader_uid': video_info.get('video_uploader_uid', ''),
                
                # æ·»åŠ è¯„è®ºç±»å‹æ ‡è¯†
                'is_main_comment': is_main_comment
            }
            
            # æ„å»ºæ‰¹é‡æ’å…¥æ•°æ®
            bulk_data.append({
                '_index': self.es_service.comment_index,
                '_id': doc['comment_id'],  # ä½¿ç”¨comment_idä½œä¸ºæ–‡æ¡£IDå®ç°å»é‡
                '_source': doc
            })
        
        # æ‰¹é‡æ’å…¥Elasticsearch
        if bulk_data:
            self.bulk_insert(bulk_data)
    
    def import_account_data(self):
        """å¯¼å…¥è´¦å·æ•°æ®ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        account_file = 'è´¦å·å¤§æ•´åˆ3.2xlsx_å·²æå–UID.csv'
        
        if os.path.exists(account_file):
            print(f"æ­£åœ¨å¤„ç†è´¦å·æ–‡ä»¶: {account_file}")
            
            # è¿™é‡Œå¯ä»¥æå–è´¦å·ä¿¡æ¯å¹¶å…³è”åˆ°è§†é¢‘/è¯„è®ºæ•°æ®
            # æˆ–è€…ä½œä¸ºç‹¬ç«‹çš„è´¦å·ç´¢å¼•å­˜å‚¨
            df = pd.read_csv(account_file, low_memory=False)
            
            # å¤„ç†è´¦å·æ•°æ®é€»è¾‘...
            print(f"è´¦å·æ•°æ®æ€»æ•°: {len(df)}")
    
    def convert_timestamp(self, timestamp):
        """è½¬æ¢æ—¶é—´æˆ³ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³"""
        if pd.isna(timestamp) or timestamp == '':
            return None
        
        try:
            # å°è¯•ä¸åŒçš„æ—¶é—´æˆ³æ ¼å¼
            if isinstance(timestamp, str):
                timestamp = timestamp.strip()
                if timestamp == '' or timestamp == 'nan':
                    return None
                    
                # å¤„ç†å­—ç¬¦ä¸²æ—¶é—´æˆ³
                if len(timestamp) == 13:  # æ¯«ç§’æ—¶é—´æˆ³
                    return int(int(timestamp) // 1000)  # è½¬æ¢ä¸ºç§’
                elif len(timestamp) == 10:  # ç§’æ—¶é—´æˆ³
                    return int(timestamp)
                else:
                    # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²
                    dt = pd.to_datetime(timestamp)
                    return int(dt.timestamp())
            else:
                # æ•°å­—æ—¶é—´æˆ³
                timestamp = int(timestamp)
                if timestamp > 1e12:  # æ¯«ç§’æ—¶é—´æˆ³
                    return int(timestamp // 1000)  # è½¬æ¢ä¸ºç§’
                else:  # ç§’æ—¶é—´æˆ³
                    return timestamp
        except Exception as e:
            print(f"æ—¶é—´æˆ³è½¬æ¢å¤±è´¥: {timestamp}, é”™è¯¯: {e}")
            return None
    
    def safe_int(self, value):
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        if pd.isna(value) or value == '':
            return 0
        try:
            return int(float(value))
        except:
            return 0
    
    def bulk_insert(self, bulk_data):
        """æ‰¹é‡æ’å…¥Elasticsearch"""
        if not bulk_data:
            return
        
        try:
            from elasticsearch.helpers import bulk
            success, failed = bulk(
                self.es_service.es,
                bulk_data,
                index=None,  # æ¯ä¸ªæ–‡æ¡£éƒ½æœ‰è‡ªå·±çš„ç´¢å¼•
                chunk_size=1000,
                request_timeout=60
            )
            print(f"æˆåŠŸæ’å…¥: {success} æ¡, å¤±è´¥: {len(failed)} æ¡")
            
            if failed:
                print(f"å¤±è´¥çš„æ–‡æ¡£: {failed[:5]}")  # åªæ‰“å°å‰5ä¸ªå¤±è´¥çš„æ–‡æ¡£
                
        except Exception as e:
            print(f"æ‰¹é‡æ’å…¥å¤±è´¥: {str(e)}")
            raise 

    def get_video_info(self, video_id):
        """æ ¹æ®è§†é¢‘IDè·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯"""
        if hasattr(self, 'video_info_mapping'):
            return self.video_info_mapping.get(str(video_id), {
                'video_title': '',
                'video_url': '',
                'video_uploader_nickname': '',
                'video_uploader_uid': ''
            })
        else:
            return {
                'video_title': '',
                'video_url': '',
                'video_uploader_nickname': '',
                'video_uploader_uid': ''
            }

    def clear_data(self, data_type='all'):
        """æ¸…ç©ºæ•°æ®"""
        try:
            if data_type in ['videos', 'all']:
                print(f"ğŸ—‘ï¸ æ¸…ç©ºè§†é¢‘ç´¢å¼•...")
                self.es_service.es.indices.delete(index=self.es_service.video_index, ignore=[400, 404])
                print(f"âœ… è§†é¢‘ç´¢å¼•å·²æ¸…ç©º")
            
            if data_type in ['comments', 'all']:
                print(f"ğŸ—‘ï¸ æ¸…ç©ºè¯„è®ºç´¢å¼•...")
                self.es_service.es.indices.delete(index=self.es_service.comment_index, ignore=[400, 404])
                print(f"âœ… è¯„è®ºç´¢å¼•å·²æ¸…ç©º")
            
            # é‡æ–°åˆ›å»ºç´¢å¼•
            if data_type in ['videos', 'comments', 'all']:
                self.es_service.create_indices()
                print(f"âœ… ç´¢å¼•é‡æ–°åˆ›å»ºå®Œæˆ")
            
            return True
        except Exception as e:
            print(f"âŒ æ¸…ç©ºæ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def get_data_statistics(self):
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        try:
            es = self.es_service.get_es_client()
            
            # è·å–è§†é¢‘ç»Ÿè®¡
            try:
                video_response = es.count(index=self.es_service.video_index)
                video_count = video_response.get('count', 0)
            except:
                video_count = 0
            
            # è·å–è¯„è®ºç»Ÿè®¡
            try:
                comment_response = es.count(index=self.es_service.comment_index)
                comment_count = comment_response.get('count', 0)
            except:
                comment_count = 0
            
            # è·å–é¡¹ç›®ç»Ÿè®¡
            try:
                projects = self.es_service.get_available_projects({'role': 'admin'})
                project_count = len(projects)
            except:
                project_count = 0
            
            return {
                'videos': video_count,
                'comments': comment_count,
                'projects': project_count,
                'total': video_count + comment_count
            }
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {
                'videos': 0,
                'comments': 0,
                'projects': 0,
                'total': 0
            }
    
    def handle_uploaded_file(self, file, file_type):
        """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶"""
        try:
            import tempfile
            import shutil
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = tempfile.mkdtemp()
            file_path = os.path.join(temp_dir, file.filename)
            
            # ä¿å­˜æ–‡ä»¶
            file.save(file_path)
            print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
            if file_type == 'account':
                result = self.process_account_file(file_path)
            elif file_type == 'video':
                result = self.process_video_file(file_path)
            elif file_type == 'comment':
                result = self.process_comment_file(file_path)
            else:
                return {'success': False, 'message': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'}
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            shutil.rmtree(temp_dir)
            
            return result
        except Exception as e:
            print(f"âŒ å¤„ç†ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
            return {'success': False, 'message': f'å¤„ç†æ–‡ä»¶å¤±è´¥: {str(e)}'}
    
    def process_account_file(self, file_path):
        """å¤„ç†è´¦å·æ–‡ä»¶"""
        try:
            import shutil
            # ç§»åŠ¨æ–‡ä»¶åˆ°å·¥ä½œç›®å½•
            target_path = 'è´¦å·å¤§æ•´åˆ3.2xlsx_å·²æå–UID.csv'
            shutil.copy2(file_path, target_path)
            
            # é‡æ–°åŠ è½½ç”¨æˆ·é¡¹ç›®æ˜ å°„
            self.user_project_mapping = None
            self.load_user_project_mapping()
            
            return {
                'success': True, 
                'message': f'è´¦å·æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±åŠ è½½ {len(self.user_project_mapping)} ä¸ªç”¨æˆ·æ˜ å°„'
            }
        except Exception as e:
            return {'success': False, 'message': f'å¤„ç†è´¦å·æ–‡ä»¶å¤±è´¥: {str(e)}'}
    
    def process_video_file(self, file_path):
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        try:
            import shutil
            # ç§»åŠ¨æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
            target_dir = '1747748467790_dbexport_209215447'
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)
            
            target_path = os.path.join(target_dir, '2025-05-20-21-41-08_EXPORT_CSV_19274722_345_bilibili_video_0.csv')
            shutil.copy2(file_path, target_path)
            
            # å¯¼å…¥è§†é¢‘æ•°æ®
            self.import_videos()
            
            return {'success': True, 'message': 'è§†é¢‘æ–‡ä»¶å¤„ç†å®Œæˆ'}
        except Exception as e:
            return {'success': False, 'message': f'å¤„ç†è§†é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}'}
    
    def process_comment_file(self, file_path):
        """å¤„ç†è¯„è®ºæ–‡ä»¶"""
        try:
            import shutil
            # ç§»åŠ¨æ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
            target_dir = '1747748467790_dbexport_209215447'
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)
            
            target_path = os.path.join(target_dir, '2025-05-20-21-41-11_EXPORT_CSV_19274722_637_bilibili_video_comment_1.csv')
            shutil.copy2(file_path, target_path)
            
            # å¯¼å…¥è¯„è®ºæ•°æ®
            self.import_comments()
            
            return {'success': True, 'message': 'è¯„è®ºæ–‡ä»¶å¤„ç†å®Œæˆ'}
        except Exception as e:
            return {'success': False, 'message': f'å¤„ç†è¯„è®ºæ–‡ä»¶å¤±è´¥: {str(e)}'}
    
    def get_visualization_statistics(self, params):
        """è·å–æ•°æ®å¯è§†åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            es = self.es_service.get_es_client()
            
            # è§£æå‚æ•°
            project_id = params.get('project_id')
            time_range = params.get('time_range')  # ä¸è®¾ç½®é»˜è®¤å€¼
            start_time = params.get('start_time')
            end_time = params.get('end_time')
            
            # æ„å»ºæ—¶é—´æŸ¥è¯¢æ¡ä»¶
            time_query = {}
            now = datetime.now()
            
            if start_time and end_time:
                time_query = {
                    "range": {
                        "create_time": {
                            "gte": int(start_time),
                            "lte": int(end_time)
                        }
                    }
                }
            elif time_range and time_range != 'custom':  # åªæœ‰æ˜ç¡®æŒ‡å®šæ—¶é—´èŒƒå›´æ—¶æ‰åº”ç”¨
                days = int(time_range.replace('d', ''))
                start_timestamp = int((now - timedelta(days=days)).timestamp())
                time_query = {
                    "range": {
                        "create_time": {
                            "gte": start_timestamp
                        }
                    }
                }
            
            # æ„å»ºé¡¹ç›®æŸ¥è¯¢æ¡ä»¶
            project_query = {}
            if project_id:
                project_query = {"term": {"project_id": project_id}}
            
            # æ„å»ºåŸºç¡€æŸ¥è¯¢
            base_query = {"bool": {"must": []}}
            if time_query:
                base_query["bool"]["must"].append(time_query)
            if project_query:
                base_query["bool"]["must"].append(project_query)
            
            # 1. æ’­æ”¾é‡ç»Ÿè®¡
            play_stats = self._get_play_stats(es, base_query)
            
            # 2. æ—¶é—´æ®µç»Ÿè®¡
            time_period_stats = self._get_time_period_stats(es, project_id)
            
            # 3. çˆ†æ–‡ç‡ç»Ÿè®¡
            viral_stats = self._get_viral_stats(es, base_query, project_id)
            
            # 4. é¡¹ç›®åˆ†å¸ƒç»Ÿè®¡
            project_stats = self._get_project_distribution(es, time_query)
            
            return {
                'play_stats': play_stats,
                'time_period_stats': time_period_stats,
                'viral_stats': viral_stats,
                'project_stats': project_stats,
                'success': True
            }
            
        except Exception as e:
            print(f"âŒ è·å–å¯è§†åŒ–ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {'success': False, 'message': str(e)}
    
    def _get_play_stats(self, es, base_query):
        """è·å–æ’­æ”¾é‡ç»Ÿè®¡ - æ”¹ä¸ºæŒ‰é¡¹ç›®çš„æ’­æ”¾æ€»é‡ç»Ÿè®¡"""
        try:
            response = es.search(
                index=self.es_service.video_index,
                body={
                    "query": base_query,
                    "aggs": {
                        "total_plays": {"sum": {"field": "video_play_count"}},
                        "avg_plays": {"avg": {"field": "video_play_count"}},
                        "max_plays": {"max": {"field": "video_play_count"}},
                        "by_project": {
                            "terms": {"field": "project_id", "size": 50},
                            "aggs": {
                                "total_plays": {"sum": {"field": "video_play_count"}},
                                "video_count": {"value_count": {"field": "video_id"}}
                            }
                        }
                    },
                    "size": 0
                }
            )
            
            aggs = response['aggregations']
            
            # æ„å»ºé¡¹ç›®æ’­æ”¾é‡æ•°æ®
            project_data = []
            project_names = []
            play_values = []
            
            for bucket in aggs['by_project']['buckets']:
                project_name = bucket['key']
                total_plays = int(bucket['total_plays']['value'] or 0)
                video_count = bucket['video_count']['value']
                
                project_data.append({
                    'name': project_name,
                    'total_plays': total_plays,
                    'video_count': video_count,
                    'avg_plays': int(total_plays / video_count) if video_count > 0 else 0
                })
                project_names.append(project_name)
                play_values.append(total_plays)
            
            # æŒ‰æ’­æ”¾é‡æ’åº
            project_data.sort(key=lambda x: x['total_plays'], reverse=True)
            
            return {
                'total_plays': int(aggs['total_plays']['value'] or 0),
                'avg_plays': int(aggs['avg_plays']['value'] or 0),
                'max_plays': int(aggs['max_plays']['value'] or 0),
                'project_data': project_data
            }
        except Exception as e:
            print(f"è·å–æ’­æ”¾é‡ç»Ÿè®¡å¤±è´¥: {e}")
            return {'total_plays': 0, 'avg_plays': 0, 'max_plays': 0, 'project_data': []}
    
    def _get_time_period_stats(self, es, project_id=None):
        """è·å–ä¸åŒæ—¶é—´æ®µçš„ç»Ÿè®¡"""
        from datetime import datetime, timedelta
        
        now = datetime.now()
        periods = {
            '1d': now - timedelta(days=1),
            '7d': now - timedelta(days=7),
            '30d': now - timedelta(days=30)
        }
        
        stats = {}
        
        for period, start_time in periods.items():
            query = {
                "bool": {
                    "must": [
                        {"range": {"create_time": {"gte": int(start_time.timestamp())}}}
                    ]
                }
            }
            
            if project_id:
                query["bool"]["must"].append({"term": {"project_id": project_id}})
            
            try:
                response = es.count(
                    index=self.es_service.video_index,
                    body={"query": query}
                )
                stats[period] = response['count']
            except:
                stats[period] = 0
        
        return stats
    
    def _get_viral_stats(self, es, base_query, project_id=None):
        """è·å–çˆ†æ–‡ç‡ç»Ÿè®¡ï¼ˆæ’­æ”¾é‡>1000ä¸ºçˆ†æ–‡ï¼‰"""
        try:
            # æ€»è§†é¢‘æ•°
            total_response = es.search(
                index=self.es_service.video_index,
                body={"query": base_query, "size": 0}
            )
            total_videos = total_response['hits']['total']['value']
            
            # çˆ†æ–‡æ•°ï¼ˆæ’­æ”¾é‡>1000ï¼‰
            viral_query = {
                "bool": {
                    "must": base_query["bool"]["must"] + [
                        {"range": {"video_play_count": {"gt": 1000}}}
                    ]
                }
            }
            
            viral_response = es.search(
                index=self.es_service.video_index,
                body={"query": viral_query, "size": 0}
            )
            viral_videos = viral_response['hits']['total']['value']
            
            # å„é¡¹ç›®çˆ†æ–‡ç‡
            project_viral_stats = {}
            if not project_id:  # åªæœ‰åœ¨ä¸é™å®šé¡¹ç›®æ—¶æ‰ç»Ÿè®¡å„é¡¹ç›®
                project_agg_response = es.search(
                    index=self.es_service.video_index,
                    body={
                        "query": base_query,
                        "aggs": {
                            "by_project": {
                                "terms": {"field": "project_id", "size": 100},
                                "aggs": {
                                    "viral_videos": {
                                        "filter": {"range": {"video_play_count": {"gt": 1000}}}
                                    }
                                }
                            }
                        },
                        "size": 0
                    }
                )
                
                for bucket in project_agg_response['aggregations']['by_project']['buckets']:
                    project_name = bucket['key']
                    total_project_videos = bucket['doc_count']
                    viral_project_videos = bucket['viral_videos']['doc_count']
                    viral_rate = (viral_project_videos / total_project_videos * 100) if total_project_videos > 0 else 0
                    
                    project_viral_stats[project_name] = {
                        'total': total_project_videos,
                        'viral': viral_project_videos,
                        'rate': round(viral_rate, 2)
                    }
            
            overall_viral_rate = (viral_videos / total_videos * 100) if total_videos > 0 else 0
            
            return {
                'total_videos': total_videos,
                'viral_videos': viral_videos,
                'viral_rate': round(overall_viral_rate, 2),
                'project_stats': project_viral_stats
            }
            
        except Exception as e:
            print(f"è·å–çˆ†æ–‡ç‡ç»Ÿè®¡å¤±è´¥: {e}")
            return {'total_videos': 0, 'viral_videos': 0, 'viral_rate': 0, 'project_stats': {}}
    
    def _get_project_distribution(self, es, time_query=None):
        """è·å–é¡¹ç›®åˆ†å¸ƒç»Ÿè®¡"""
        try:
            query = {"bool": {"must": []}}
            if time_query:
                query["bool"]["must"].append(time_query)
            
            response = es.search(
                index=self.es_service.video_index,
                body={
                    "query": query,
                    "aggs": {
                        "by_project": {
                            "terms": {"field": "project_id", "size": 100},
                            "aggs": {
                                "total_plays": {"sum": {"field": "video_play_count"}}
                            }
                        }
                    },
                    "size": 0
                }
            )
            
            project_data = []
            for bucket in response['aggregations']['by_project']['buckets']:
                project_data.append({
                    'name': bucket['key'],
                    'video_count': bucket['doc_count'],
                    'total_plays': int(bucket['total_plays']['value'] or 0)
                })
            
            return sorted(project_data, key=lambda x: x['video_count'], reverse=True)
            
        except Exception as e:
            print(f"è·å–é¡¹ç›®åˆ†å¸ƒå¤±è´¥: {e}")
            return [] 